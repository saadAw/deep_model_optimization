{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark_notebook_setup.py\n",
    "# This is a helper file to make the notebook cleaner. It will contain our functions.\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "\n",
    "def check_prerequisites():\n",
    "    \"\"\"Checks if trtexec is installed and available in the system's PATH.\"\"\"\n",
    "    print(\"Checking for 'trtexec'...\")\n",
    "    if shutil.which(\"trtexec\") is None:\n",
    "        print(\"❌ ERROR: 'trtexec' command not found.\")\n",
    "        raise RuntimeError(\"trtexec not found. Cannot proceed with benchmarking.\")\n",
    "    print(\"✅ 'trtexec' is available.\")\n",
    "\n",
    "def export_to_onnx(model_path, onnx_path, num_classes, batch_size, input_name, input_shape):\n",
    "    \"\"\"Loads a PyTorch model and exports it to ONNX format.\"\"\"\n",
    "    # ... (This function is correct, no changes needed) ...\n",
    "    print(f\"  Exporting '{os.path.basename(model_path)}' to ONNX...\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"  ❌ ERROR: Model file not found at {model_path}\")\n",
    "        return False\n",
    "    try:\n",
    "        shape_parts = [int(d) for d in input_shape.split('x')]\n",
    "        dummy_input_shape = (batch_size, *shape_parts)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = torchvision.models.resnet50(weights=None, num_classes=num_classes)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval().to(device)\n",
    "        dummy_input = torch.randn(dummy_input_shape, device=device)\n",
    "        torch.onnx.export(\n",
    "            model, dummy_input, onnx_path, export_params=True, opset_version=13,\n",
    "            do_constant_folding=True, input_names=[input_name], output_names=['output'],\n",
    "            dynamic_axes={input_name: {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "        )\n",
    "        print(f\"  ✅ Successfully exported to {onnx_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ FAILED to export to ONNX: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_benchmark(onnx_path, engine_path, json_path, log_path, config, is_sparse):\n",
    "    \"\"\"Constructs and runs the trtexec benchmark command.\"\"\"\n",
    "    # ... (This function is correct from the last fix, no changes needed) ...\n",
    "    print(f\"  Building and benchmarking with trtexec...\")\n",
    "    opt_shape_str = f\"{config['INPUT_NAME']}:{config['BATCH_SIZE']}x{config['INPUT_SHAPE']}\"\n",
    "    command = [\n",
    "        \"trtexec\", f\"--onnx={onnx_path}\", \"--fp16\", \"--useCudaGraph\",\n",
    "        f\"--optShapes={opt_shape_str}\", f\"--iterations={config['ITERATIONS']}\",\n",
    "        f\"--duration={config['DURATION']}\", f\"--saveEngine={engine_path}\",\n",
    "        f\"--exportTimes={json_path}\",\n",
    "    ]\n",
    "    if is_sparse:\n",
    "        command.append(\"--sparsity=enable\")\n",
    "    command_str = \" \".join(command) + f\" > {log_path} 2>&1\"\n",
    "    print(f\"  Running command: {command_str}\")\n",
    "    try:\n",
    "        subprocess.run(command_str, shell=True, check=True)\n",
    "        print(f\"  ✅ trtexec benchmark completed. Log saved to {log_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  ❌ trtexec FAILED.\")\n",
    "        print(f\"  Return code: {e.returncode}\")\n",
    "        print(f\"  Check the full log file for details: {log_path}\")\n",
    "        return False\n",
    "\n",
    "def parse_results(json_path, batch_size):\n",
    "    \"\"\"Parses the JSON output from trtexec to get key performance metrics.\"\"\"\n",
    "    # ===================================================================\n",
    "    # --- THIS IS THE CORRECTED FUNCTION ---\n",
    "    # ===================================================================\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            # The JSON file might contain multiple JSON objects. We only need the last one.\n",
    "            # Read all lines and find the last valid JSON object.\n",
    "            json_text = \"\"\n",
    "            for line in f:\n",
    "                # A simple heuristic: the results object starts with '{'\n",
    "                if line.strip().startswith('{'):\n",
    "                    json_text = line\n",
    "            \n",
    "            # If no line started with '{', something is wrong.\n",
    "            if not json_text:\n",
    "                print(f\"  ⚠️ Warning: Could not find a valid JSON object in {json_path}\")\n",
    "                return None\n",
    "            \n",
    "            data = json.loads(json_text)\n",
    "\n",
    "        # Now, safely access the keys\n",
    "        results_dict = data.get(\"results\", {})\n",
    "        \n",
    "        throughput_qps = results_dict.get(\"throughput(qps)\", 0)\n",
    "        images_per_second = throughput_qps * batch_size\n",
    "\n",
    "        latency_ms_list = results_dict.get(\"latency\", [0])\n",
    "        median_latency_ms = sorted(latency_ms_list)[len(latency_ms_list) // 2]\n",
    "\n",
    "        gpu_compute_ms_list = results_dict.get(\"GPU-compute\", [0])\n",
    "        median_gpu_compute_ms = sorted(gpu_compute_ms_list)[len(gpu_compute_ms_list) // 2]\n",
    "        \n",
    "        return {\n",
    "            \"Throughput (images/sec)\": images_per_second,\n",
    "            \"Batch Latency (ms)\": median_latency_ms,\n",
    "            \"GPU Compute Time (ms)\": median_gpu_compute_ms\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error parsing {json_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- Main Configuration Dictionary ---\n",
    "CONFIG = {\n",
    "    # --- Model & Data Parameters ---\n",
    "    \"NUM_CLASSES\": 1000,  \n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"INPUT_NAME\": \"input\", \n",
    "    \"INPUT_SHAPE\": \"3x224x224\", # CxHxW\n",
    "\n",
    "    # --- Paths ---\n",
    "    # The base directory where your 'saved_models_and_logs' folder is located.\n",
    "    # Assumes you are running this notebook from `/workspace`.\n",
    "    \"BASE_PATH\": \"/workspace\",\n",
    "    \n",
    "    # Relative paths to your model files\n",
    "    \"DENSE_MODEL_RELATIVE_PATH\": \"saved_models_and_logs/resnet50_baseline/resnet50_baseline_ft_imagenetmini_final.pth\",\n",
    "    \"SPARSE_MODEL_RELATIVE_PATH\": \"saved_models_and_logs/pruning_nm_sparsity/resnet50_prune_nm24_ft/model_final.pth\",\n",
    "    \n",
    "    # Where all generated files (ONNX, engines, logs, JSON) will be saved.\n",
    "    \"OUTPUT_DIR_NAME\": \"benchmark_results\",\n",
    "\n",
    "    # --- trtexec Parameters ---\n",
    "    \"ITERATIONS\": 500,\n",
    "    \"DURATION\": 10,\n",
    "}\n",
    "\n",
    "# --- Construct Absolute Paths ---\n",
    "CONFIG[\"DENSE_MODEL_PATH\"] = os.path.join(CONFIG[\"BASE_PATH\"], CONFIG[\"DENSE_MODEL_RELATIVE_PATH\"])\n",
    "CONFIG[\"SPARSE_MODEL_PATH\"] = os.path.join(CONFIG[\"BASE_PATH\"], CONFIG[\"SPARSE_MODEL_RELATIVE_PATH\"])\n",
    "CONFIG[\"OUTPUT_DIR\"] = os.path.join(CONFIG[\"BASE_PATH\"], CONFIG[\"OUTPUT_DIR_NAME\"])\n",
    "\n",
    "# --- Display Configuration for Verification ---\n",
    "print(\"--- Configuration Loaded ---\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:<30}: {value}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(CONFIG[\"OUTPUT_DIR\"], exist_ok=True)\n",
    "print(f\"Results will be saved in: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78011f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the helper functions we just wrote to the .py file\n",
    "from benchmark_notebook_setup import check_prerequisites, export_to_onnx, run_benchmark, parse_results\n",
    "\n",
    "# Run the check\n",
    "try:\n",
    "    check_prerequisites()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Processing DENSE Model ---\")\n",
    "\n",
    "# Define paths for the dense model's artifacts\n",
    "dense_onnx_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"resnet50_dense.onnx\")\n",
    "dense_engine_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"resnet50_dense.engine\")\n",
    "dense_json_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"results_dense.json\")\n",
    "dense_log_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"benchmark_dense.log\")\n",
    "\n",
    "# Step 1: Export to ONNX\n",
    "if export_to_onnx(CONFIG[\"DENSE_MODEL_PATH\"], dense_onnx_path, CONFIG[\"NUM_CLASSES\"], CONFIG[\"BATCH_SIZE\"], CONFIG[\"INPUT_NAME\"], CONFIG[\"INPUT_SHAPE\"]):\n",
    "    # Step 2: Run Benchmark (is_sparse=False)\n",
    "    run_benchmark(dense_onnx_path, dense_engine_path, dense_json_path, dense_log_path, CONFIG, is_sparse=False)\n",
    "\n",
    "print(\"\\n--- DENSE Model Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Processing N:M SPARSE Model ---\")\n",
    "\n",
    "# Define paths for the sparse model's artifacts\n",
    "sparse_onnx_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"resnet50_sparse.onnx\")\n",
    "sparse_engine_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"resnet50_sparse.engine\")\n",
    "sparse_json_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"results_sparse.json\")\n",
    "sparse_log_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"benchmark_sparse.log\")\n",
    "\n",
    "# Step 1: Export to ONNX\n",
    "if export_to_onnx(CONFIG[\"SPARSE_MODEL_PATH\"], sparse_onnx_path, CONFIG[\"NUM_CLASSES\"], CONFIG[\"BATCH_SIZE\"], CONFIG[\"INPUT_NAME\"], CONFIG[\"INPUT_SHAPE\"]):\n",
    "    # Step 2: Run Benchmark (is_sparse=True)\n",
    "    run_benchmark(sparse_onnx_path, sparse_engine_path, sparse_json_path, sparse_log_path, CONFIG, is_sparse=True)\n",
    "\n",
    "print(\"\\n--- SPARSE Model Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558403af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Analysis Script (Corrected for JSON Array Format) ---\n",
      "Parsing array-formatted results from: /workspace/benchmark_results/results_dense.json\n",
      "  ✅ Successfully parsed and calculated statistics.\n",
      "Parsing array-formatted results from: /workspace/benchmark_results/results_sparse.json\n",
      "  ✅ Successfully parsed and calculated statistics.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e9c0a th {\n",
       "  text-align: left;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_e9c0a td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e9c0a_row0_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #5fba7d 16.7%, transparent 16.7%);\n",
       "}\n",
       "#T_e9c0a_row1_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #5fba7d 25.6%, transparent 25.6%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e9c0a\">\n",
       "  <caption><h2>Benchmark Summary (Batch Size: 32)</h2></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e9c0a_level0_col0\" class=\"col_heading level0 col0\" >Throughput (images/sec)</th>\n",
       "      <th id=\"T_e9c0a_level0_col1\" class=\"col_heading level0 col1\" >Batch Latency (ms)</th>\n",
       "      <th id=\"T_e9c0a_level0_col2\" class=\"col_heading level0 col2\" >GPU Compute Time (ms)</th>\n",
       "      <th id=\"T_e9c0a_level0_col3\" class=\"col_heading level0 col3\" >Speedup (Throughput)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e9c0a_level0_row0\" class=\"row_heading level0 row0\" >Dense Model</th>\n",
       "      <td id=\"T_e9c0a_row0_col0\" class=\"data row0 col0\" >3,916.38</td>\n",
       "      <td id=\"T_e9c0a_row0_col1\" class=\"data row0 col1\" >7.67 ms</td>\n",
       "      <td id=\"T_e9c0a_row0_col2\" class=\"data row0 col2\" >6.18 ms</td>\n",
       "      <td id=\"T_e9c0a_row0_col3\" class=\"data row0 col3\" >1.00x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9c0a_level0_row1\" class=\"row_heading level0 row1\" >N:M Sparse Model</th>\n",
       "      <td id=\"T_e9c0a_row1_col0\" class=\"data row1 col0\" >4,126.49</td>\n",
       "      <td id=\"T_e9c0a_row1_col1\" class=\"data row1 col1\" >7.08 ms</td>\n",
       "      <td id=\"T_e9c0a_row1_col2\" class=\"data row1 col2\" >5.59 ms</td>\n",
       "      <td id=\"T_e9c0a_row1_col3\" class=\"data row1 col3\" >1.05x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FINAL ANALYSIS CELL (Corrected for JSON Array Format)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"--- Final Analysis Script (Corrected for JSON Array Format) ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 32\n",
    "OUTPUT_DIR = \"/workspace/benchmark_results\"\n",
    "\n",
    "# --- Define paths to the existing JSON files ---\n",
    "dense_json_path = os.path.join(OUTPUT_DIR, \"results_dense.json\")\n",
    "sparse_json_path = os.path.join(OUTPUT_DIR, \"results_sparse.json\")\n",
    "\n",
    "def parse_results_from_array(json_path, batch_size):\n",
    "    \"\"\"\n",
    "    This new function correctly parses a JSON file that is an array of run data.\n",
    "    It calculates the summary statistics (throughput, median latency) manually.\n",
    "    \"\"\"\n",
    "    print(f\"Parsing array-formatted results from: {json_path}\")\n",
    "    if not os.path.exists(json_path) or os.path.getsize(json_path) == 0:\n",
    "        print(f\"  ❌ Error: File not found or is empty.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            # The entire file is a single JSON array\n",
    "            run_data = json.load(f)\n",
    "\n",
    "        if not isinstance(run_data, list) or not run_data:\n",
    "            print(\"  ❌ Error: JSON content is not a non-empty list.\")\n",
    "            return None\n",
    "\n",
    "        # --- Calculate Metrics Manually ---\n",
    "        \n",
    "        # 1. Latency and GPU Compute Time\n",
    "        latencies = [run.get('latencyMs', 0) for run in run_data]\n",
    "        gpu_times = [run.get('computeMs', 0) for run in run_data]\n",
    "        \n",
    "        median_latency_ms = sorted(latencies)[len(latencies) // 2]\n",
    "        median_gpu_compute_ms = sorted(gpu_times)[len(gpu_times) // 2]\n",
    "\n",
    "        # 2. Throughput\n",
    "        # Total time is from the start of the first inference to the end of the last one.\n",
    "        first_run_start_ms = run_data[0].get('startH2dMs', 0)\n",
    "        last_run_end_ms = run_data[-1].get('endD2hMs', 0)\n",
    "        total_duration_s = (last_run_end_ms - first_run_start_ms) / 1000.0\n",
    "\n",
    "        total_images = len(run_data) * batch_size\n",
    "        images_per_second = total_images / total_duration_s if total_duration_s > 0 else 0\n",
    "        \n",
    "        print(\"  ✅ Successfully parsed and calculated statistics.\")\n",
    "        return {\n",
    "            \"Throughput (images/sec)\": images_per_second,\n",
    "            \"Batch Latency (ms)\": median_latency_ms,\n",
    "            \"GPU Compute Time (ms)\": median_gpu_compute_ms\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ An unexpected error occurred while parsing {json_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Analysis Logic ---\n",
    "dense_results = parse_results_from_array(dense_json_path, BATCH_SIZE)\n",
    "sparse_results = parse_results_from_array(sparse_json_path, BATCH_SIZE)\n",
    "\n",
    "if not dense_results or not sparse_results:\n",
    "    print(\"\\n❌ Could not generate comparison. One or both result files failed to parse. Please check errors above.\")\n",
    "else:\n",
    "    df = pd.DataFrame([dense_results, sparse_results], index=['Dense Model', 'N:M Sparse Model'])\n",
    "    \n",
    "    throughput_speedup = df.loc['N:M Sparse Model', 'Throughput (images/sec)'] / df.loc['Dense Model', 'Throughput (images/sec)'] if df.loc['Dense Model', 'Throughput (images/sec)'] != 0 else float('inf')\n",
    "    latency_speedup = df.loc['Dense Model', 'Batch Latency (ms)'] / df.loc['N:M Sparse Model', 'Batch Latency (ms)'] if df.loc['N:M Sparse Model', 'Batch Latency (ms)'] != 0 else float('inf')\n",
    "\n",
    "    df['Speedup (Throughput)'] = [1.0, throughput_speedup]\n",
    "    \n",
    "    styled_df = df.style.format({\n",
    "        'Throughput (images/sec)': '{:,.2f}',\n",
    "        'Batch Latency (ms)': '{:.2f} ms',\n",
    "        'GPU Compute Time (ms)': '{:.2f} ms',\n",
    "        'Speedup (Throughput)': '{:.2f}x',\n",
    "    }).bar(subset=['Speedup (Throughput)'], align='left', color=['#d65f5f', '#5fba7d'], vmin=0.9, vmax=max(1.5, df['Speedup (Throughput)'].max())) \\\n",
    "      .set_caption(f\"<h2>Benchmark Summary (Batch Size: {BATCH_SIZE})</h2>\") \\\n",
    "      .set_table_styles([{'selector': 'th', 'props': [('text-align', 'left'), ('font-weight', 'bold')]},\n",
    "                         {'selector': 'td', 'props': [('text-align', 'left')]}])\n",
    "      \n",
    "    display(HTML(styled_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb860f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
