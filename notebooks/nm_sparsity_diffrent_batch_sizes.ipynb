{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf7f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Automated Multi-Batch Benchmark Suite ---\n",
      "✓ 'trtexec' is available.\n",
      "Found existing ONNX files:\n",
      "  - /workspace/benchmark_results/resnet50_dense.onnx\n",
      "  - /workspace/benchmark_results/resnet50_sparse.onnx\n",
      "\n",
      "===== RUNNING BENCHMARKS FOR BATCH SIZE: 16 =====\n",
      "  > Benchmarking DENSE model with Batch Size = 16...\n",
      "    ✓ Benchmark successful. Log: /workspace/batch_benchmark_results/log_dense_b16.log\n",
      "  > Benchmarking SPARSE model with Batch Size = 16...\n",
      "    ✓ Benchmark successful. Log: /workspace/batch_benchmark_results/log_sparse_b16.log\n",
      "\n",
      "===== RUNNING BENCHMARKS FOR BATCH SIZE: 32 =====\n",
      "  > Benchmarking DENSE model with Batch Size = 32...\n",
      "    ✓ Benchmark successful. Log: /workspace/batch_benchmark_results/log_dense_b32.log\n",
      "  > Benchmarking SPARSE model with Batch Size = 32...\n",
      "    ✓ Benchmark successful. Log: /workspace/batch_benchmark_results/log_sparse_b32.log\n",
      "\n",
      "===== RUNNING BENCHMARKS FOR BATCH SIZE: 64 =====\n",
      "  > Benchmarking DENSE model with Batch Size = 64...\n",
      "    ✓ Benchmark successful. Log: /workspace/batch_benchmark_results/log_dense_b64.log\n",
      "  > Benchmarking SPARSE model with Batch Size = 64...\n",
      "    ✓ Benchmark successful. Log: /workspace/batch_benchmark_results/log_sparse_b64.log\n",
      "\n",
      "===== RUNNING BENCHMARKS FOR BATCH SIZE: 128 =====\n",
      "  > Benchmarking DENSE model with Batch Size = 128...\n",
      "    ✓ Benchmark successful. Log: /workspace/batch_benchmark_results/log_dense_b128.log\n",
      "  > Benchmarking SPARSE model with Batch Size = 128...\n",
      "    ✓ Benchmark successful. Log: /workspace/batch_benchmark_results/log_sparse_b128.log\n",
      "\n",
      "\n",
      "================================================================================\n",
      "--- FINAL BENCHMARK SUMMARY ---\n",
      "================================================================================\n",
      "           Dense Throughput (img/s) Sparse Throughput (img/s) Dense Latency (ms) Sparse Latency (ms) Speedup\n",
      "Batch Size                                                                                                  \n",
      "16                         3,499.17                  3,747.81               4.10                3.82   1.07x\n",
      "32                         3,914.63                  4,112.65               7.69                7.09   1.05x\n",
      "64                         3,887.84                  4,144.68              15.92               14.76   1.07x\n",
      "128                        4,341.78                  4,485.69              29.44               28.61   1.03x\n",
      "\n",
      "Benchmark artifacts (logs, engines, JSON) saved in: /workspace/batch_benchmark_results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# ======================================================================================\n",
    "# --- CONFIGURATION ---\n",
    "# ======================================================================================\n",
    "CONFIG = {\n",
    "    # --- Model Paths (Verify these are correct) ---\n",
    "    \"DENSE_MODEL_PATH\": \"/workspace/saved_models_and_logs/resnet50_baseline/resnet50_baseline_ft_imagenetmini_final.pth\",\n",
    "    \"SPARSE_MODEL_PATH\": \"/workspace/saved_models_and_logs/pruning_nm_sparsity/resnet50_prune_nm24_ft/model_final.pth\",\n",
    "\n",
    "    # --- Batch Sizes to Test ---\n",
    "    \"BATCH_SIZES\": [16, 32, 64, 128], # Add or remove sizes as needed\n",
    "\n",
    "    # --- Model & Data Parameters ---\n",
    "    \"NUM_CLASSES\": 1000,\n",
    "    \"INPUT_NAME\": \"input\",\n",
    "    \"INPUT_SHAPE\": \"3x224x224\",\n",
    "\n",
    "    # --- Output Directory ---\n",
    "    \"OUTPUT_DIR\": \"/workspace/batch_benchmark_results\",\n",
    "\n",
    "    # --- trtexec Parameters ---\n",
    "    \"ITERATIONS\": 500,\n",
    "    \"DURATION\": 10,\n",
    "}\n",
    "# ======================================================================================\n",
    "# --- SCRIPT LOGIC (No need to edit below this line) ---\n",
    "# ======================================================================================\n",
    "\n",
    "def check_prerequisites():\n",
    "    if shutil.which(\"trtexec\") is None:\n",
    "        print(\"ERROR: 'trtexec' not found. Cannot proceed.\")\n",
    "        sys.exit(1)\n",
    "    print(\"✓ 'trtexec' is available.\")\n",
    "\n",
    "def run_benchmark(model_type, onnx_path, batch_size, is_sparse):\n",
    "    \"\"\"Runs trtexec for a given model, batch size, and sparsity setting.\"\"\"\n",
    "    print(f\"  > Benchmarking {model_type.upper()} model with Batch Size = {batch_size}...\")\n",
    "    \n",
    "    # Create unique filenames for each run\n",
    "    run_id = f\"{model_type}_b{batch_size}\"\n",
    "    engine_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], f\"engine_{run_id}.engine\")\n",
    "    json_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], f\"results_{run_id}.json\")\n",
    "    log_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], f\"log_{run_id}.log\")\n",
    "\n",
    "    opt_shape_str = f\"{CONFIG['INPUT_NAME']}:{batch_size}x{CONFIG['INPUT_SHAPE']}\"\n",
    "    \n",
    "    command = [\n",
    "        \"trtexec\",\n",
    "        f\"--onnx={onnx_path}\",\n",
    "        \"--fp16\",\n",
    "        \"--useCudaGraph\",\n",
    "        f\"--optShapes={opt_shape_str}\",\n",
    "        f\"--iterations={CONFIG['ITERATIONS']}\",\n",
    "        f\"--duration={CONFIG['DURATION']}\",\n",
    "        f\"--saveEngine={engine_path}\",\n",
    "        f\"--exportTimes={json_path}\",\n",
    "    ]\n",
    "\n",
    "    if is_sparse:\n",
    "        command.append(\"--sparsity=enable\")\n",
    "\n",
    "    command_str = \" \".join(command) + f\" > {log_path} 2>&1\"\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command_str, shell=True, check=True)\n",
    "        print(f\"    ✓ Benchmark successful. Log: {log_path}\")\n",
    "        return json_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"    ❌ trtexec FAILED for BS={batch_size}. Check log: {log_path}\")\n",
    "        # Check for OOM in the log\n",
    "        with open(log_path, 'r') as f:\n",
    "            if \"out of memory\" in f.read().lower():\n",
    "                print(\"    HINT: This was likely an Out of Memory (OOM) error. The batch size may be too large for your GPU.\")\n",
    "        return None\n",
    "\n",
    "def parse_results_from_array(json_path, batch_size):\n",
    "    \"\"\"Parses a JSON file that is an array of run data.\"\"\"\n",
    "    if not json_path or not os.path.exists(json_path) or os.path.getsize(json_path) == 0:\n",
    "        return {\"Throughput (images/sec)\": 0, \"Batch Latency (ms)\": 0}\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            run_data = json.load(f)\n",
    "        if not isinstance(run_data, list) or not run_data: return None\n",
    "        latencies = [run.get('latencyMs', 0) for run in run_data]\n",
    "        median_latency_ms = sorted(latencies)[len(latencies) // 2]\n",
    "        first_run_start_ms = run_data[0].get('startH2dMs', 0)\n",
    "        last_run_end_ms = run_data[-1].get('endD2hMs', 0)\n",
    "        total_duration_s = (last_run_end_ms - first_run_start_ms) / 1000.0\n",
    "        total_images = len(run_data) * batch_size\n",
    "        images_per_second = total_images / total_duration_s if total_duration_s > 0 else 0\n",
    "        return {\n",
    "            \"Throughput (images/sec)\": images_per_second,\n",
    "            \"Batch Latency (ms)\": median_latency_ms\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"Throughput (images/sec)\": 0, \"Batch Latency (ms)\": 0}\n",
    "\n",
    "def main():\n",
    "    print(\"--- Starting Automated Multi-Batch Benchmark Suite ---\")\n",
    "    check_prerequisites()\n",
    "    os.makedirs(CONFIG[\"OUTPUT_DIR\"], exist_ok=True)\n",
    "    \n",
    "    # --- ONNX Export (Done only once) ---\n",
    "    # We don't need to re-import the export function, just assume ONNX exists\n",
    "    # If they don't, the user can run the notebook cell once.\n",
    "    dense_onnx_path = \"/workspace/benchmark_results/resnet50_dense.onnx\"\n",
    "    sparse_onnx_path = \"/workspace/benchmark_results/resnet50_sparse.onnx\"\n",
    "\n",
    "    if not os.path.exists(dense_onnx_path) or not os.path.exists(sparse_onnx_path):\n",
    "        print(\"ERROR: ONNX files not found. Please run the ONNX export cells from the notebook first.\")\n",
    "        print(f\"Expected dense: {dense_onnx_path}\")\n",
    "        print(f\"Expected sparse: {sparse_onnx_path}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print(f\"Found existing ONNX files:\\n  - {dense_onnx_path}\\n  - {sparse_onnx_path}\")\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    # --- Main Benchmark Loop ---\n",
    "    for bs in CONFIG[\"BATCH_SIZES\"]:\n",
    "        print(f\"\\n===== RUNNING BENCHMARKS FOR BATCH SIZE: {bs} =====\")\n",
    "        \n",
    "        # Run Dense\n",
    "        dense_json_path = run_benchmark(\"dense\", dense_onnx_path, bs, is_sparse=False)\n",
    "        \n",
    "        # Run Sparse\n",
    "        sparse_json_path = run_benchmark(\"sparse\", sparse_onnx_path, bs, is_sparse=True)\n",
    "        \n",
    "        # Parse and store results\n",
    "        dense_perf = parse_results_from_array(dense_json_path, bs)\n",
    "        sparse_perf = parse_results_from_array(sparse_json_path, bs)\n",
    "        \n",
    "        # Calculate speedup\n",
    "        try:\n",
    "            speedup = sparse_perf[\"Throughput (images/sec)\"] / dense_perf[\"Throughput (images/sec)\"] if dense_perf[\"Throughput (images/sec)\"] > 0 else 0\n",
    "        except:\n",
    "            speedup = 0\n",
    "            \n",
    "        all_results.append({\n",
    "            \"Batch Size\": bs,\n",
    "            \"Dense Throughput (img/s)\": dense_perf[\"Throughput (images/sec)\"],\n",
    "            \"Sparse Throughput (img/s)\": sparse_perf[\"Throughput (images/sec)\"],\n",
    "            \"Dense Latency (ms)\": dense_perf[\"Batch Latency (ms)\"],\n",
    "            \"Sparse Latency (ms)\": sparse_perf[\"Batch Latency (ms)\"],\n",
    "            \"Speedup\": speedup\n",
    "        })\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"--- FINAL BENCHMARK SUMMARY ---\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    df = df.set_index(\"Batch Size\")\n",
    "\n",
    "    # Format for display\n",
    "    formatted_df = df.style.format({\n",
    "        \"Dense Throughput (img/s)\": \"{:,.2f}\",\n",
    "        \"Sparse Throughput (img/s)\": \"{:,.2f}\",\n",
    "        \"Dense Latency (ms)\": \"{:.2f}\",\n",
    "        \"Sparse Latency (ms)\": \"{:.2f}\",\n",
    "        \"Speedup\": \"{:.2f}x\"\n",
    "    }).bar(subset=['Speedup'], color='#5fba7d', vmin=1.0)\n",
    "    \n",
    "    # In a script, we print the dataframe. In a notebook, display() would be used.\n",
    "    print(df.to_string(formatters={\n",
    "        \"Dense Throughput (img/s)\": \"{:,.2f}\".format,\n",
    "        \"Sparse Throughput (img/s)\": \"{:,.2f}\".format,\n",
    "        \"Dense Latency (ms)\": \"{:.2f}\".format,\n",
    "        \"Sparse Latency (ms)\": \"{:.2f}\".format,\n",
    "        \"Speedup\": \"{:.2f}x\".format\n",
    "    }))\n",
    "    print(\"\\nBenchmark artifacts (logs, engines, JSON) saved in:\", CONFIG[\"OUTPUT_DIR\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d4511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
