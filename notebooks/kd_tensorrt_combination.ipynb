{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9047d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.3.0a0+6ddf5cf85e.nv24.04\n",
      "Torch-TensorRT Version: 2.3.0a0\n",
      "Is CUDA available? True\n",
      "CUDA device: NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ===================================================================\n",
    "#                      IMPORTS AND SETUP\n",
    "# ===================================================================\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch_tensorrt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Environment Setup for Performance ---\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torch-TensorRT Version: {torch_tensorrt.__version__}\")\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA is not available. TensorRT compilation requires a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc3cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base Directory inside container: /workspace\n",
      "✅ Model will be loaded from: /workspace/saved_models_and_logs/knowledge_distillation/resnet50_to_resnet18pretrained_kd/model_final.pth\n",
      "✅ Optimized models will be saved to: /workspace/saved_models_and_logs/kd_tensorrt\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ===================================================================\n",
    "#                      USER CONFIGURATION\n",
    "# ===================================================================\n",
    "# Define paths as they exist INSIDE the container.\n",
    "BASE_DIR = Path('/workspace')\n",
    "\n",
    "# --- ‼️ IMPORTANT: UPDATE THIS PATH ‼️ ---\n",
    "# Path to your fine-tuned and KNOWLEDGE-DISTILLED ResNet-18 model file\n",
    "MODEL_PATH = BASE_DIR / 'saved_models_and_logs' / 'knowledge_distillation' / 'resnet50_to_resnet18pretrained_kd' / 'model_final.pth' \n",
    "\n",
    "# Path to save the optimized TensorRT models.\n",
    "OPTIMIZED_MODEL_DIR = BASE_DIR / 'saved_models_and_logs' / 'kd_tensorrt'\n",
    "# ===================================================================\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "OPTIMIZED_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Base Directory inside container: {BASE_DIR}\")\n",
    "print(f\"✅ Model will be loaded from: {MODEL_PATH}\")\n",
    "print(f\"✅ Optimized models will be saved to: {OPTIMIZED_MODEL_DIR}\")\n",
    "\n",
    "# Verify that the model file actually exists at that path\n",
    "if not MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Model file not found inside container at: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a331787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Loading the custom distilled ResNet-18 model...\n",
      "✅ Custom distilled ResNet-18 model loaded and moved to GPU successfully.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ===================================================================\n",
    "#                LOAD THE DISTILLED RESNET-18 MODEL\n",
    "# ===================================================================\n",
    "\n",
    "print(\"-> Loading the custom distilled ResNet-18 model...\")\n",
    "\n",
    "# 1. Instantiate the ResNet-18 model architecture.\n",
    "#    Ensure num_classes matches the output of your distilled model.\n",
    "model = models.resnet18(num_classes=1000)\n",
    "\n",
    "# 2. Load the state dictionary from your .pth file.\n",
    "state_dict = torch.load(MODEL_PATH, map_location='cpu')\n",
    "\n",
    "# 3. Load the weights into the model.\n",
    "#    This handles various ways a model checkpoint might be saved.\n",
    "if 'model_state_dict' in state_dict:\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "elif 'state_dict' in state_dict:\n",
    "    model.load_state_dict(state_dict['state_dict'])\n",
    "else:\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "# 4. Set the model to evaluation mode and move to GPU.\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "print(\"✅ Custom distilled ResNet-18 model loaded and moved to GPU successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a19f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt._compile:Module was provided as a torch.nn.Module, trying to script the module with torch.jit.script. In the event of a failure please preconvert your module to TorchScript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Compiling model with Torch-TensorRT (FP32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Detected and removing exception in TorchScript IR for node:  = prim::If(%460) # <string>:5:2  block0():    -> ()  block1():     = prim::RaiseException(%390, %self.conv1.bias) # <string>:5:2    -> ()\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Compilation complete.\n",
      "✅ FP32 optimized model saved to: /workspace/saved_models_and_logs/kd_tensorrt/resnet18_distilled_trt_fp32.ts\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ===================================================================\n",
    "#             COMPILE AND SAVE TENSORRT FP32 MODEL\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Compiling model with Torch-TensorRT (FP32)...\")\n",
    "\n",
    "try:\n",
    "    # Use a representative batch size for compilation.\n",
    "    # This determines the static batch size of the resulting engine.\n",
    "    trt_model_fp32 = torch_tensorrt.compile(\n",
    "        model,\n",
    "        inputs=[torch_tensorrt.Input((32, 3, 224, 224), dtype=torch.float32)],\n",
    "        enabled_precisions={torch.float32},\n",
    "        workspace_size=1 << 28,  # 256MB workspace\n",
    "        ir=\"torchscript\"\n",
    "    )\n",
    "    print(\"✅ Compilation complete.\")\n",
    "\n",
    "    # Save the FP32 optimized model\n",
    "    fp32_model_path = OPTIMIZED_MODEL_DIR / 'resnet18_distilled_trt_fp32.ts'\n",
    "    torch.jit.save(trt_model_fp32, fp32_model_path)\n",
    "    print(f\"✅ FP32 optimized model saved to: {fp32_model_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ ERROR during FP32 compilation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0375c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt._compile:Module was provided as a torch.nn.Module, trying to script the module with torch.jit.script. In the event of a failure please preconvert your module to TorchScript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Compiling model with Torch-TensorRT (FP16)...\n",
      "✅ Compilation complete.\n",
      "✅ FP16 optimized model saved to: /workspace/saved_models_and_logs/kd_tensorrt/resnet18_distilled_trt_fp16.ts\n",
      "\n",
      "🎉 All tasks complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Detected and removing exception in TorchScript IR for node:  = prim::If(%460) # <string>:5:2  block0():    -> ()  block1():     = prim::RaiseException(%390, %self.conv1.bias) # <string>:5:2    -> ()\n",
      "WARNING: [Torch-TensorRT] - For input x.1, found user specified input dtype as Half, however when inspecting the graph, the input type expected was inferred to be Float\n",
      "The compiler is going to use the user setting Half\n",
      "This conflict may cause an error at runtime due to partial compilation being enabled and therefore\n",
      "compatibility with PyTorch's data type convention is required.\n",
      "If you do indeed see errors at runtime either:\n",
      "- Remove the dtype spec for x.1\n",
      "- Disable partial compilation by setting require_full_compilation to True\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Check verbose logs for the list of affected weights.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 24 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 13 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ===================================================================\n",
    "#             COMPILE AND SAVE TENSORRT FP16 MODEL\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Compiling model with Torch-TensorRT (FP16)...\")\n",
    "\n",
    "try:\n",
    "    # Compile for FP16 precision\n",
    "    trt_model_fp16 = torch_tensorrt.compile(\n",
    "        model,\n",
    "        inputs=[torch_tensorrt.Input((32, 3, 224, 224), dtype=torch.float16)],\n",
    "        enabled_precisions={torch.float16},\n",
    "        workspace_size=1 << 28,  # 256MB workspace\n",
    "        ir=\"torchscript\"\n",
    "    )\n",
    "    print(\"✅ Compilation complete.\")\n",
    "\n",
    "    # Save the FP16 optimized model\n",
    "    fp16_model_path = OPTIMIZED_MODEL_DIR / 'resnet18_distilled_trt_fp16.ts'\n",
    "    torch.jit.save(trt_model_fp16, fp16_model_path)\n",
    "    print(f\"✅ FP16 optimized model saved to: {fp16_model_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ ERROR during FP16 compilation: {e}\")\n",
    "\n",
    "print(\"\\n🎉 All tasks complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dbac15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
